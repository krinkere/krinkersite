{"pages":[{"tags":"pages","text":"Why I started this blog My background My family My hobbies","title":"About","url":"pages/about.html"},{"tags":"How To article","text":"While cleaning data in csv file, it is often common to see entity name such as city, person name, organization, etc being misspelled a little slightly and hence not producing same type of statistic as you might want. For example, let's say that you try to collect stats on number of times user accessed your page. In that user name can be entered manually or it provided by different systems, then it might be different slight. Here I mean that we need to ensure that case is the same not to throw our stats and any spaces are cleaned as well just as a prelim step. Also, the username might be slightly misspelled with an extra dash, space or single character. Here lays the danger though, there can we two usernames that vary just by a character. I remember the days of hotmail and that you would often get emails sent to blackwolf to your black.wolf account. Gmail saw this as a problem and now it ignores periods in the emails to avoid this issue. What I am trying to say is that you need to use typos with care. Now how you do it? First of all, clean the text from spaces and make it lowercased. import pandas as pd # read in our dataset df = pd . read_csv ( \"my_data.csv\" ) # convert to lower case df [ 'username' ] = df [ 'username' ] . str . lower () # remove trailing white spaces df [ 'username' ] = df [ 'username' ] . str . strip () # let's take a look at list of unique usernames values username = df [ 'username' ] . unique () # sort them alphabetically and then take a closer look username . sort () print ( username ) We should see some usernames that seem too close of a match... let's try to find all usernames that are more that 90% close to each other and replace them to the most common name import fuzzywuzzy # function to replace rows in the provided column of the provided dataframe # that match the provided string above the provided ratio with the provided string def replace_matches_in_column ( df , column , string_to_match , min_ratio = 90 ): # get a list of unique strings strings = df [ column ] . unique () # get the top 10 closest matches to our input string matches = fuzzywuzzy . process . extract ( string_to_match , strings , limit = 10 , scorer = fuzzywuzzy . fuzz . token_sort_ratio ) # only get matches with a ratio > 90 close_matches = [ matches [ 0 ] for matches in matches if matches [ 1 ] >= min_ratio ] # get the rows of all the close matches in our dataframe rows_with_matches = df [ column ] . isin ( close_matches ) # replace all rows with close matches with the input matches df . loc [ rows_with_matches , column ] = string_to_match # let us know the function's done print ( \"All done!\" ) Now you can call it as such # use the function we just wrote to replace close matches to \"d.i khan\" with \"d.i khan\" replace_matches_in_column ( df = df , column = 'username' , string_to_match = \"black.wolf\" ) All done!","title":"How to remove typos from entity names via fuzzywuzzy module in python.","url":"fix_spelling_errors.html"},{"tags":"How To article","text":"In my line of work, I have to deal with a lot of spreadsheets coming my way with different type of data. I don't control these csv files, hence I never know how they are being generated. If I were to simply read the file, I would often get something like that. UnicodeDecodeError Traceback (most recent call last) pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens() pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_with_dtype() pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._string_convert() pandas/_libs/parsers.pyx in pandas._libs.parsers._string_box_utf8() UnicodeDecodeError: 'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte Basically, when you specify the following, you assume that the information was encoded in UTF-8 ()default) format data = pd . read_csv ( \"my_data.csv\" ) However, if that's not the case and format is not UTF-8 then you get a nasty error shown previously. What to do? Try manually some common encoders, or look at the file and try to figure it out? A much better way is to use chardet module to do it for you. Here we going to read first ten thousand bytes to figure out the encoding type. Note that chardet is not 100% accurate and you would actually see the level of confidence of encoder detection as part of chardet output. But it is still better than guessing manually. # look at the first ten thousand bytes to guess the character encoding with open ( \"my_data.csv\" , 'rb' ) as rawdata : result = chardet . detect ( rawdata . read ( 10000 )) # check what the character encoding might be print ( result ) The result is {'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''} So chardet is 73% confidence that the right encoding is \"Windows-1252\". Now we can use this data to specify encoding type as we trying to read the file data = pd . read_csv ( \"my_data.csv\" , encoding = 'Windows-1252' )) No errors!","title":"How to detect encoding of CSV file in python","url":"encoding_csv_file_python.html"},{"tags":"How To article","text":"When trying to update your local copy from remote master copy, you will see following error $ git pull origin master error: Your local changes to the following files would be overwritten by merge: <list of files> Please commit your changes or stash them before you merge. Aborting You have several options here Commit the change $ git add . $ git commit -m \"committing before the update\" Stash them $ git stash $ git stash pop Overwrite local changes git reset --hard","title":"Commit your changes or stash them before you can merge??!?!","url":"Commit_your_changes_or_stash_them_before_you_can_merge.html"},{"tags":"How To article","text":"Git does not let you to check in an empty folder, even if you are using it as a temp output location. How to work around it? In the folder that you are trying to commit, create .gitignore file and add following content &ast; &ast;/ !.gitignore then add it to the git $ git add .gitignore The &ast; line tells git to ignore all files in the folder, but !.gitignore tells git to still include the .gitignore file. This way, your local repository and any other clones of the repository all get both the empty folder and the .gitignore it needs. May be obvious but also add &ast;/ to the git ignore to also ignore sub folders.","title":"How to check in a folder, but ignore its contents","url":"check_in_folder_ignore_contents.html"},{"tags":"How To article","text":"Say you are developing a new feature and you realize after few commits that you went off to a way different route that you suppose to and you need to back it up few commits and start over... this definitely would be a cleaner way vs trying to remove what was done manually. How to do it though? Check out what you want and get rid of all that code... $ git reset --hard 0d3b7ac32 Then you would push it up $ git push origin +master Pretty simple once you know it.","title":"How to back off back to the earlier version of the code","url":"back_it_off.html"},{"tags":"How To article","text":"Imagine the situation where you wrote your code and then decided to add it to your git repo. Pretty easy right? $ git init $ git add . Before you commit, you want to see what's going to be committed. So you do $ git status Now you see whole bunch of config and target files that have no business being in the repo. Not a problem, you can use .gitignore right? First remove what you added, create .gitignore file and you can re add again only source files. $ git rm -r . create .gitignore with /target/&ast;&ast; .settings/&ast;&ast; .classpath .project and re-add $ git add . Check what's about to be committed... and what?!?!? old files? How can this be? Did I messed up my regex? spelled gitignore wrong or forgot the leading period? Nope, everything seems correct... After reading gitignore help guide... you need to clear your cache!!! Here is what you do $ git rm -r --cached . cached flag is the key difference. After this command, re-add, verify and finally commit: $ git add . $ git commit -m \"source files only!!!\"","title":"How to remove files added to git","url":"How_to_remove_files_added_to_git.html"},{"tags":"Hardware hacks","text":"Before my PC seized to be, I used to have old wired printer that did the job... the only problem was that I already have few Apple laptops and I wanted to be able to print from them (The problem is easily solved by buying wireless printer that supports AirPrint, i.e. even print from your iPhone!). Anyway, if you have wired Windows printer and don't want to upgrade just yet here is what you need to do: On Windows PC 1. Establish user account on your PC. This was one thing that I had to do to make everything that should work to actually work. This is as easy as opening your control panel and clicking on Add user in your Users menu. For more tricks see this: http://www.howtogeek.com/howto/10325/manage-user-accounts-in-windows-home-server/ 2. Now onto actual set up... Select Start->Devices and Printers. Right click on the printer that you want to share, and either pick share or properties and then pick sharing tab. Make sure that share check box is selected and make sure that you note down the name of the printer. 3. Open command prompt. Use ipconfig command to find your PC's IP. To summarize this part. You have IP address and name of the printer to connect to and you have the credentials that you created in step 1. On MAC 1. Open System Preferences and locate Printers and Scanners icon. Click! 2. Select + under printers to add new printer, i.e. wired Windows printer. 3. Right click on the menu and select Customize Toolbar and add Advanced 4. Click on Advanced. For type select Windows printer via spoolss 5. For URL provide IP and printer name that you have... so the link looks like smb://192.138.1.13/printer_name (Replace any spaces with %20 in your 6. Under Choose a driver or Printer Model pick your printer type (I did not see my exact model so I picked closes HP model instead. Worked) 7. At this point your PC printer will be connected to your Mac. Testing Select something to print on your Mac... in my case, the first time it tried to print it asked for my PC username/password which we created previously, after that it was stored in my keychain and was never an issue again. That's it! Hope it helps and once again, let me know if you have further questions, etc.","title":"Accessing wired Windows printer from your Mac","url":"Accessing_wired_Windows_printer_from_your_Mac.html"},{"tags":"Tutorial","text":"Several good books already explain what Lucene scoring really means and how it is calculated in great detail with lots of basic concepts explain. In this, post I am going to try to keep it high level for people already familiar with the basics and go straight for the scoring overview. The factors involved in Lucene's scoring algorithm are as follows: 1. tf = term frequency in document = measure of how often a term appears in the document 2. idf = inverse document frequency = measure of how often the term appears across the index 3. coord = number of terms in the query that were found in the document 4. lengthNorm = measure of the importance of a term according to the total number of terms in the field 5. queryNorm = normalization factor so that queries can be compared 6. boost (index) = boost of the field at index-time 7. boost (query) = boost of the field at query-time The implementation, implication and rationales of factors 1,2, 3 and 4 in DefaultSimilarity.java, which is what you get if you don't explicitly specify a similarity, are: note: the implication of these factors should be read as, \"Everything else being equal, … \" tf Implementation: sqrt(freq) Implication: the more frequent a term occurs in a document, the greater its score Rationale: documents which contains more of a term are generally more relevant idf Implementation: log(numDocs/(docFreq+1)) + 1 Implication: the greater the occurrence of a term in different documents, the lower its score Rationale: common terms are less important than uncommon ones coord Implementation: overlap / maxOverlap Implication: of the terms in the query, a document that contains more terms will have a higher score Rationale: self-explanatory lengthNorm Implementation: 1/sqrt(numTerms) Implication: a term matched in fields with less terms have a higher score Rationale: a term in a field with less terms is more important than one with more queryNorm is not related to the relevance of the document, but rather tries to make scores between different queries comparable. It is implemented as 1/sqrt(sumOfSquaredWeights) So, roughly speaking (quoting Mark Harwood from the mailing list), Documents containing all the search terms are good Matches on rare words are better than for common words Long documents are not as good as short ones Documents which mention the search terms many times are good The mathematical definition of the scoring can be found in org.apache.lucene.search.Class Similarity Customizing scoring Its easy to customize the scoring algorithm. Just subclass DefaultSimilarity and override the method you want to customize. For example, if you want to ignore how common a term appears across the index, Similarity sim = new DefaultSimilarity () { public float idf ( int i , int i1 ) { return 1 ; } } and if you think for the title field, more terms is better Similarity sim = new DefaultSimilarity () { public float lengthNorm ( String field , int numTerms ) { if ( field . equals ( \"title\" )) return ( float ) ( 0.1 * Math . log ( numTerms )); else return super . lengthNorm ( field , numTerms ); } }","title":"Lucene scoring examplained","url":"Lucene_scoring_examplained.html"},{"tags":"How To article","text":"Here is a quick example: $ grep -r \"text string to search\" directory-path To search for a string ‘logged in' in all text (*.log) files located in /etc/networks/ directory for example, use: $ grep \"logged in\" /etc/networks/*.log To search all subdirectories recursively, include -r option like so: $ grep -r \"logged in\" /etc/networks/*.log The grep command prints the matching lines for each match. Pass -H option to print the filename only: $ grep -H -r \"logged in\" /etc/networks/*.log To search for two or more words, use egrep: egrep -w -r 'logged in|logged out' /etc/networks/*.log To hide warning spam of permission for certain directories being denied, etc, send them to dev/null: $ grep -w -r 'logged in|logged out' /etc/networks/*.log 2 >/dev/null To make it case insensitive, use -i option: $ grep -i \"logged in\" /etc/networks/*.log","title":"How to find a file containing particular text in Linux","url":"How_to_find_a_file_containing_particular_text_in_Linux.html"}]}