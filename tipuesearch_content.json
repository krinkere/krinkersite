{"pages":[{"title":"About","tags":"pages","url":"pages/about.html","text":"Why I started this blog My background My family My hobbies"},{"title":"Commit your changes or stash them before you can merge??!?!","tags":"How To article","url":"Commit_your_changes_or_stash_them_before_you_can_merge.html","text":"When trying to update your local copy from remote master copy, you will see following error $ git pull origin master error: Your local changes to the following files would be overwritten by merge: <list of files> Please commit your changes or stash them before you merge. Aborting You have several options here Commit the change $ git add . $ git commit -m \"committing before the update\" Stash them $ git stash $ git stash pop Overwrite local changes git reset --hard"},{"title":"Lucene scoring examplained","tags":"Tutorial","url":"Lucene_scoring_examplained.html","text":"Several good books already explain what Lucene scoring really means and how it is calculated in great detail with lots of basic concepts explain. In this, post I am going to try to keep it high level for people already familiar with the basics and go straight for the scoring overview. The factors involved in Lucene's scoring algorithm are as follows: 1. tf = term frequency in document = measure of how often a term appears in the document 2. idf = inverse document frequency = measure of how often the term appears across the index 3. coord = number of terms in the query that were found in the document 4. lengthNorm = measure of the importance of a term according to the total number of terms in the field 5. queryNorm = normalization factor so that queries can be compared 6. boost (index) = boost of the field at index-time 7. boost (query) = boost of the field at query-time The implementation, implication and rationales of factors 1,2, 3 and 4 in DefaultSimilarity.java, which is what you get if you don't explicitly specify a similarity, are: note: the implication of these factors should be read as, \"Everything else being equal, … \" tf Implementation: sqrt(freq) Implication: the more frequent a term occurs in a document, the greater its score Rationale: documents which contains more of a term are generally more relevant idf Implementation: log(numDocs/(docFreq+1)) + 1 Implication: the greater the occurrence of a term in different documents, the lower its score Rationale: common terms are less important than uncommon ones coord Implementation: overlap / maxOverlap Implication: of the terms in the query, a document that contains more terms will have a higher score Rationale: self-explanatory lengthNorm Implementation: 1/sqrt(numTerms) Implication: a term matched in fields with less terms have a higher score Rationale: a term in a field with less terms is more important than one with more queryNorm is not related to the relevance of the document, but rather tries to make scores between different queries comparable. It is implemented as 1/sqrt(sumOfSquaredWeights) So, roughly speaking (quoting Mark Harwood from the mailing list), Documents containing all the search terms are good Matches on rare words are better than for common words Long documents are not as good as short ones Documents which mention the search terms many times are good The mathematical definition of the scoring can be found in org.apache.lucene.search.Class Similarity Customizing scoring Its easy to customize the scoring algorithm. Just subclass DefaultSimilarity and override the method you want to customize. For example, if you want to ignore how common a term appears across the index, Similarity sim = new DefaultSimilarity () { public float idf ( int i , int i1 ) { return 1 ; } } and if you think for the title field, more terms is better Similarity sim = new DefaultSimilarity () { public float lengthNorm ( String field , int numTerms ) { if ( field . equals ( \"title\" )) return ( float ) ( 0.1 * Math . log ( numTerms )); else return super . lengthNorm ( field , numTerms ); } }"},{"title":"How to find a file containing particular text in Linux","tags":"How To article","url":"How_to_find_a_file_containing_particular_text_in_Linux.html","text":"Here is a quick example: $ grep -r \"text string to search\" directory-path To search for a string ‘logged in' in all text (*.log) files located in /etc/networks/ directory for example, use: $ grep \"logged in\" /etc/networks/*.log To search all subdirectories recursively, include -r option like so: $ grep -r \"logged in\" /etc/networks/*.log The grep command prints the matching lines for each match. Pass -H option to print the filename only: $ grep -H -r \"logged in\" /etc/networks/*.log To search for two or more words, use egrep: egrep -w -r 'logged in|logged out' /etc/networks/*.log To hide warning spam of permission for certain directories being denied, etc, send them to dev/null: $ grep -w -r 'logged in|logged out' /etc/networks/*.log 2 >/dev/null To make it case insensitive, use -i option: $ grep -i \"logged in\" /etc/networks/*.log"}]}