{"pages":[{"text":"Why I started this blog My background My family My hobbies","title":"About","url":"pages/about.html","tags":"pages"},{"text":"I was recently working on an instance of MariaDB where I loaded almost 200GB of data. My /var folder filled up almost to 90% and I got all sorts of warning logs about size limitations, etc. Before it turned into bigger issue, I decided to take proactive approach and move my data to another location. Useful command to check the size $ sudo du -sch * Verify current data directory location $ mysql -u root -p Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 4 Server version: 10.1.33-MariaDB MariaDB Server Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]> select @@datadir; +--------------------------+ | @@datadir | +--------------------------+ | /var/lib/mysql/ | +--------------------------+ 1 row in set (0.00 sec) MariaDB [(none)]> exit Bye Stop running mariadb instance $ sudo systemctl stop mariadb $ sudo systemctl status mariadb Double check if instance is running (Force kill any rogue mysql processes) and check if port 3306 is still occupied $ ps -elf | grep mysql $ netstat -apn | grep 3306 Move data to new location and back up old instance $ sudo rsync -av /var/lib/mysql /mnt/big_data $ sudo mv /var/lib/mysql /var/lib/mysql.bak Edit configuration files to change the defaults. \"/etc/my.cnf\" [mysqld] datadir=/mnt/big_data/mysql/ socket=/mnt/big_data/mysql/mysql.sock \"/etc/my.cnf.d/client.cnf\" [client] port=3306 socket=/mnt/big_data/mysql/mysql.sock I also had to chmod new directory, otherwise mariadb was failing since it could not write to the new location. Start it back up and make sure that new directory is in use $ sudo systemctl start mariadb $ sudo systemctl status mariadb $ mysql -u root -p MariaDB [(none)]> select @@datadir; +--------------------------+ | @@datadir | +--------------------------+ | /mnt/big_data/mysql/ | +--------------------------+ 1 row in set (0.00 sec) Partial Reference","title":"How to move MariaDB data to another location","url":"move_mariadb_data.html","tags":"How To article"},{"text":"Enable Google Yum repository Create a file called /etc/yum.repos.d/google-chrome.repo and add the following lines of code to it. [google-chrome] name = google-chrome baseurl = http://dl.google.com/linux/chrome/rpm/stable/$basearch enabled = 1 gpgcheck = 1 gpgkey = https://dl-ssl.google.com/linux/linux_signing_key.pub Install key -Refer to this guide for more details. $ wget -- no - check - certificate https : // dl . google . com / linux / linux_signing_key . pub $ sudo rpm -- import linux_signing_key.pub Install Google Chrome $ sudo yum install google-chrome-stable Start Google Chrome $ google-chrome &","title":"How to install Google Chrome on your Linux System","url":"install_chrome_on_linux.html","tags":"How To article"},{"text":"Suppose you're very indecisive, so whenever you want to watch a movie, you ask your friend Willow if she thinks you'll like it. In order to answer, Willow first needs to figure out what movies you like, so you give her a bunch of movies and tell her whether you liked each one or not (i.e., you give her a labeled training set). Then, when you ask her if she thinks you'll like movie X or not, she plays a 20 questions-like game with IMDB, asking questions like \"Is X a romantic movie?\", \"Does Johnny Depp star in X?\", and so on. She asks more informative questions first (i.e., she maximizes the information gain of each question), and gives you a yes/no answer at the end. Thus, Willow is a decision tree for your movie preferences. But Willow is only human, so she doesn't always generalize your preferences very well (i.e., she overfits). In order to get more accurate recommendations, you'd like to ask a bunch of your friends, and watch movie X if most of them say they think you'll like it. That is, instead of asking only Willow, you want to ask Woody, Apple, and Cartman as well, and they vote on whether you'll like a movie (i.e., you build an ensemble classifier , aka a forest in this case). Now you don't want each of your friends to do the same thing and give you the same answer, so you first give each of them slightly different data. After all, you're not absolutely sure of your preferences yourself you told Willow you loved Titanic, but maybe you were just happy that day because it was your birthday, so maybe some of your friends shouldn't use the fact that you liked Titanic in making their recommendations. Or maybe you told her you loved Cinderella, but actually you really really loved it, so some of your friends should give Cinderella more weight. So instead of giving your friends the same data you gave Willow, you give them slightly perturbed versions. You don't change your love/hate decisions, you just say you love/hate some movies a little more or less (formally, you give each of your friends a bootstrapped version of your original training data). For example, whereas you told Willow that you liked Black Swan and Harry Potter and disliked Avatar, you tell Woody that you liked Black Swan so much you watched it twice, you disliked Avatar, and don't mention Harry Potter at all. By using this ensemble, you hope that while each of your friends gives somewhat idiosyncratic recommendations (Willow thinks you like vampire movies more than you do, Woody thinks you like Pixar movies, and Cartman thinks you just hate everything), the errors get canceled out in the majority. Thus, your friends now form a bagged (bootstrap aggregated) forest of your movie preferences. There's still one problem with your data, however. While you loved both Titanic and Inception, it wasn't because you like movies that star Leonardio DiCaprio. Maybe you liked both movies for other reasons. Thus, you don't want your friends to all base their recommendations on whether Leo is in a movie or not. So when each friend asks IMDB a question, only a random subset of the possible questions is allowed (i.e., when you're building a decision tree, at each node you use some randomness in selecting the attribute to split on , say by randomly selecting an attribute or by selecting an attribute from a random subset). This means your friends aren't allowed to ask whether Leonardo DiCaprio is in the movie whenever they want. So whereas previously you injected randomness at the data level, by perturbing your movie preferences slightly, now you're injecting randomness at the model level, by making your friends ask different questions at different times. And so your friends now form a random forest .","title":"Layman's Introduction to Random Forests","url":"layman_random_forrest.html","tags":"Reference"},{"text":"Visit Spark to download tgz version of spark with hadoop Unzip and move it to /opt directory $ tar -xzf spark-2.3.0-bin-hadoop2.7.tgz $ mv spark-2.3.0-bin-hadoop2.7 /opt/spark-2.3.0 Create sym link $ ln -s /opt/spark-2.3.0 /opt/spark Add it to bash export SPARK_HOME=/opt/spark export PATH=$SPARK_HOME/bin:$PATH At this point Spark is installed on your machine. Test it $ pyspark Welcome to ____ __ / __/__ ___ _____/ /__ _ \\ \\/ _ \\/ _ ` / __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.3.0 /_/ Using Python version 3.6.4 (default, Jan 16 2018 18:10:19) SparkSession available as ' spark ' . Connect it to your python scripts, by installing findspark to point python to the location of the Spark $ pip install findspark Install pyspark to be able to use Spark from python $ pip install pyspark Now you will be able to use Spark from your python scripts import findspark import pyspark from pyspark.sql import SQLContext , Row findspark . init ( spark_home = \"/opt/spark\" ) conf = pyspark . SparkConf () . setAppName ( 'tf_fraud' ) sc = pyspark . SparkContext ( conf = conf ) sqlctx = SQLContext ( sc ) connection_url = \"jdbc:oracle:thin:\" + username + \"/\" + password + \"@\" + ip + \":\" + str ( port ) + \"/\" + SID df_pyspark = sqlctx . read . format ( 'jdbc' ) . options ( url = connection_url , dbtable = \"employee\" , driver = \"oracle.jdbc.OracleDriver\" ) . load () print ( df_pyspark . printSchema ()) cad_ser_nums = df_pyspark . foreach ( print )","title":"How to install Spark and use it from python via pyspark","url":"install_spark.html","tags":"How To article"},{"text":"Not sure if this step is needed since Oracle Instant Client suppose to be back compatible, but I went ahead and installed client that matched Oracle instance that I was trying to connect to Determine Oracle version by running this command. Mine was 12.1.0.2.0 SELECT * FROM V$VERSION Download Oracle Instant Client . I have Linux 86-64 so I used this link to download RPM version. Install downloaded oracle instant client sudo yum install oracle-instantclient12.1-basic-12.1.0.2.0-1.x86_64.rpm Add oracle instant client to your PATH. Permanently since I don't have any other Oracle products that might have broken.: sudo sh -c \"echo /usr/lib/oracle/12.1/client64/lib > /etc/ld.so.conf.d/oracle-instantclient.conf\" sudo ldconfig At this point your environment is set. Now it is time to connect to it by installing cx_Oracle library in your python. python -m pip install cx_Oracle --upgrade That should do it. Now write a simple python script to get the version of the Oracle database that you are trying to connect to. import cx_Oracle ip = 'specify_ip_or_hostname' port = 1234 SID = 'specify_sid_or_schema' dsn_tns = cx_Oracle . makedsn ( ip , port , SID ) # You might get these via environment variable to make thing secure username = 'username' password = 'password' conn = cx_Oracle . connect ( username , password , dsn_tns ) print ( conn . version ) conn . close () >>> 12.1.0.2.0 References: Installing cx_Oracle on Linux Quick cx_Oracle Install","title":"How to connect to remote Oracle database from your python script","url":"connect_to_oracle_via_python.html","tags":"How To article"},{"text":".table-borders td, .table-borders th { border: 1px solid black; padding: 10px; } span.bold-red { color: red; font-weight: bold; } SQL Pandas select * from table_name df select * from table_name limit 3 df.head(3) select col_name_1 from table_name where col_name_2 = 'value' df[df.col_name_2 == 'value'].col_name_1 select distinct col_name_1 from table_name df.col_name_1.unique() select * from table_name where col_name_1 = 'val_1' and col_name_2 = 'val_2' df[(df.col_name_1 == 'val_1') & (df.col_name_2 == 'val_2')] select col_name_1, col_name_3, col_name_3 from table_name where col_name_4 = 'val_1' and col_name_5 = 'val_2' df[(df.col_name_4 == 'val_1') & (df.col_name_5 == 'val_2')][['col_name_1', 'col_name_2', 'col_name_3']] select * from table_name where col_name_1 = 'value' order by col_name_2 df[df.col_name_1 == 'value'].sort_values('col_name_2') select * from table_name where col_name_1 = 'value' order by col_name_2 desc df[df.col_name_1 == 'value'].sort_values('col_name_2', ascending=False) select * from table_name where col_name in ('val_1', 'val_2') df[df.col_name.isin(['val_1', 'val_2'])] select * from table_name where col_name not in ('val_1', 'val_2') df[~df.col_name.isin(['val_1', 'val_2'])] select col_name_1, col_name_2, count(&ast;) from table_name group by col_name_1, col_name_2 order by col_name_1, col_name_2 df.groupby(['col_name_1', 'col_name_2']).size() select col_name_1, col_name_2, count(&ast;) from table_name group by col_name_1, col_name_2 order by col_name_1, count(&ast;) desc df.groupby(['col_name_1', 'col_name_2']).size().to_frame('size').reset_index().sort_values(['col_name_1', 'size'], ascending=[True, False]) select col_name_1, count(&ast;) from table_name where col_name_2 = 'val_1' group by col_name_1 having count(&ast;) > 1000 order by count(&ast;) desc df[df.col_name_2 == 'val_1'].groupby('col_name_1').filter(lambda g: len(g) > 1000).groupby('col_name_1').size().sort_values(ascending=False) select col_name from table_name order by size desc limit 10 df.nlargest(10, columns='col_name') select col_name from table_name order by size desc limit 10 offset 10 df.nlargest( 20 , columns='col_name').tail(10) select max (col_name), min (col_name), mean (col_name), median (col_name) from table_name df.agg({'col_name': ['min', 'max', 'mean', 'median']}) select col_name_1, col_name_2, col_name_3, col_name_4 from table_name_1 join table_name_2 on table_name_1.col_name_id_1 = table_name_2.col_name_id_2 where table_name_2.col_name = 'val' df1.merge(df2[df2.col_name == 'val'][['col_name_id_2']], left_on='col_name_id_1', right_on='col_name_id_2', how='inner')[['col_name_1', 'col_name_2', 'col_name_3', 'col_name_4']] create table table_name (col_name_1 integer, col_name_2 text); insert into table_name values (1, 'val_1'); insert into table_name values (2, 'val_2'); insert into table_name values (3, 'val_3'); df1 = pd.DataFrame({'id': [1, 2], 'name': ['val_1', 'val_2']}) df2 = pd.DataFrame({'id': [3], 'name': ['val_3']}) pd.concat([df1, df2]).reset_index(drop=True) update table_name set col_name_1 = 'val_1' where col_name_2 == 'val_2' df.loc[df['col_name_2'] == 'val_2', 'col_name_1'] = 'val_1' delete from table_name where col_name = 'val' df = df[df.col_name != 'val' df.drop(df[df.col_name == 'val'].index)","title":"Mapping SQL to Pandas","url":"mapping_sql_to_pandas.html","tags":"Python"},{"text":"While cleaning data in csv file, it is often common to see entity name such as city, person name, organization, etc being misspelled a little slightly and hence not producing same type of statistic as you might want. For example, let's say that you try to collect stats on number of times user accessed your page. In that user name can be entered manually or it provided by different systems, then it might be different slight. Here I mean that we need to ensure that case is the same not to throw our stats and any spaces are cleaned as well just as a prelim step. Also, the username might be slightly misspelled with an extra dash, space or single character. Here lays the danger though, there can we two usernames that vary just by a character. I remember the days of hotmail and that you would often get emails sent to blackwolf to your black.wolf account. Gmail saw this as a problem and now it ignores periods in the emails to avoid this issue. What I am trying to say is that you need to use typos with care. Now how you do it? First of all, clean the text from spaces and make it lowercased. import pandas as pd # read in our dataset df = pd . read_csv ( \"my_data.csv\" ) # convert to lower case df [ 'username' ] = df [ 'username' ] . str . lower () # remove trailing white spaces df [ 'username' ] = df [ 'username' ] . str . strip () # let's take a look at list of unique usernames values username = df [ 'username' ] . unique () # sort them alphabetically and then take a closer look username . sort () print ( username ) We should see some usernames that seem too close of a match... let's try to find all usernames that are more that 90% close to each other and replace them to the most common name import fuzzywuzzy # function to replace rows in the provided column of the provided dataframe # that match the provided string above the provided ratio with the provided string def replace_matches_in_column ( df , column , string_to_match , min_ratio = 90 ): # get a list of unique strings strings = df [ column ] . unique () # get the top 10 closest matches to our input string matches = fuzzywuzzy . process . extract ( string_to_match , strings , limit = 10 , scorer = fuzzywuzzy . fuzz . token_sort_ratio ) # only get matches with a ratio > 90 close_matches = [ matches [ 0 ] for matches in matches if matches [ 1 ] >= min_ratio ] # get the rows of all the close matches in our dataframe rows_with_matches = df [ column ] . isin ( close_matches ) # replace all rows with close matches with the input matches df . loc [ rows_with_matches , column ] = string_to_match # let us know the function's done print ( \"All done!\" ) Now you can call it as such # use the function we just wrote to replace close matches to \"d.i khan\" with \"d.i khan\" replace_matches_in_column ( df = df , column = 'username' , string_to_match = \"black.wolf\" ) All done!","title":"How to remove typos from entity names via fuzzywuzzy module in python.","url":"fix_spelling_errors.html","tags":"How To article"},{"text":"In my line of work, I have to deal with a lot of spreadsheets coming my way with different type of data. I don't control these csv files, hence I never know how they are being generated. If I were to simply read the file, I would often get something like that. UnicodeDecodeError Traceback (most recent call last) pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens() pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_with_dtype() pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._string_convert() pandas/_libs/parsers.pyx in pandas._libs.parsers._string_box_utf8() UnicodeDecodeError: 'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte Basically, when you specify the following, you assume that the information was encoded in UTF-8 ()default) format data = pd . read_csv ( \"my_data.csv\" ) However, if that's not the case and format is not UTF-8 then you get a nasty error shown previously. What to do? Try manually some common encoders, or look at the file and try to figure it out? A much better way is to use chardet module to do it for you. Here we going to read first ten thousand bytes to figure out the encoding type. Note that chardet is not 100% accurate and you would actually see the level of confidence of encoder detection as part of chardet output. But it is still better than guessing manually. # look at the first ten thousand bytes to guess the character encoding with open ( \"my_data.csv\" , 'rb' ) as rawdata : result = chardet . detect ( rawdata . read ( 10000 )) # check what the character encoding might be print ( result ) The result is {'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''} So chardet is 73% confidence that the right encoding is \"Windows-1252\". Now we can use this data to specify encoding type as we trying to read the file data = pd . read_csv ( \"my_data.csv\" , encoding = 'Windows-1252' )) No errors!","title":"How to detect encoding of CSV file in python","url":"encoding_csv_file_python.html","tags":"How To article"},{"text":"When trying to update your local copy from remote master copy, you will see following error $ git pull origin master error: Your local changes to the following files would be overwritten by merge: <list of files> Please commit your changes or stash them before you merge. Aborting You have several options here Commit the change $ git add . $ git commit -m \"committing before the update\" Stash them $ git stash $ git stash pop Overwrite local changes git reset --hard","title":"Commit your changes or stash them before you can merge??!?!","url":"Commit_your_changes_or_stash_them_before_you_can_merge.html","tags":"How To article"},{"text":"Git does not let you to check in an empty folder, even if you are using it as a temp output location. How to work around it? In the folder that you are trying to commit, create .gitignore file and add following content &ast; &ast;/ !.gitignore then add it to the git $ git add .gitignore The &ast; line tells git to ignore all files in the folder, but !.gitignore tells git to still include the .gitignore file. This way, your local repository and any other clones of the repository all get both the empty folder and the .gitignore it needs. May be obvious but also add &ast;/ to the git ignore to also ignore sub folders.","title":"How to check in a folder, but ignore its contents","url":"check_in_folder_ignore_contents.html","tags":"How To article"},{"text":"Say you are developing a new feature and you realize after few commits that you went off to a way different route that you suppose to and you need to back it up few commits and start over... this definitely would be a cleaner way vs trying to remove what was done manually. How to do it though? Check out what you want and get rid of all that code... $ git reset --hard 0d3b7ac32 Then you would push it up $ git push origin +master Pretty simple once you know it.","title":"How to back off back to the earlier version of the code","url":"back_it_off.html","tags":"How To article"},{"text":"Imagine the situation where you wrote your code and then decided to add it to your git repo. Pretty easy right? $ git init $ git add . Before you commit, you want to see what's going to be committed. So you do $ git status Now you see whole bunch of config and target files that have no business being in the repo. Not a problem, you can use .gitignore right? First remove what you added, create .gitignore file and you can re add again only source files. $ git rm -r . create .gitignore with /target/&ast;&ast; .settings/&ast;&ast; .classpath .project and re-add $ git add . Check what's about to be committed... and what?!?!? old files? How can this be? Did I messed up my regex? spelled gitignore wrong or forgot the leading period? Nope, everything seems correct... After reading gitignore help guide... you need to clear your cache!!! Here is what you do $ git rm -r --cached . cached flag is the key difference. After this command, re-add, verify and finally commit: $ git add . $ git commit -m \"source files only!!!\"","title":"How to remove files added to git","url":"How_to_remove_files_added_to_git.html","tags":"How To article"},{"text":"Before my PC seized to be, I used to have old wired printer that did the job... the only problem was that I already have few Apple laptops and I wanted to be able to print from them (The problem is easily solved by buying wireless printer that supports AirPrint, i.e. even print from your iPhone!). Anyway, if you have wired Windows printer and don't want to upgrade just yet here is what you need to do: On Windows PC 1. Establish user account on your PC. This was one thing that I had to do to make everything that should work to actually work. This is as easy as opening your control panel and clicking on Add user in your Users menu. For more tricks see this: http://www.howtogeek.com/howto/10325/manage-user-accounts-in-windows-home-server/ 2. Now onto actual set up... Select Start->Devices and Printers. Right click on the printer that you want to share, and either pick share or properties and then pick sharing tab. Make sure that share check box is selected and make sure that you note down the name of the printer. 3. Open command prompt. Use ipconfig command to find your PC's IP. To summarize this part. You have IP address and name of the printer to connect to and you have the credentials that you created in step 1. On MAC 1. Open System Preferences and locate Printers and Scanners icon. Click! 2. Select + under printers to add new printer, i.e. wired Windows printer. 3. Right click on the menu and select Customize Toolbar and add Advanced 4. Click on Advanced. For type select Windows printer via spoolss 5. For URL provide IP and printer name that you have... so the link looks like smb://192.138.1.13/printer_name (Replace any spaces with %20 in your 6. Under Choose a driver or Printer Model pick your printer type (I did not see my exact model so I picked closes HP model instead. Worked) 7. At this point your PC printer will be connected to your Mac. Testing Select something to print on your Mac... in my case, the first time it tried to print it asked for my PC username/password which we created previously, after that it was stored in my keychain and was never an issue again. That's it! Hope it helps and once again, let me know if you have further questions, etc.","title":"Accessing wired Windows printer from your Mac","url":"Accessing_wired_Windows_printer_from_your_Mac.html","tags":"Hardware hacks"},{"text":"Several good books already explain what Lucene scoring really means and how it is calculated in great detail with lots of basic concepts explain. In this, post I am going to try to keep it high level for people already familiar with the basics and go straight for the scoring overview. The factors involved in Lucene's scoring algorithm are as follows: 1. tf = term frequency in document = measure of how often a term appears in the document 2. idf = inverse document frequency = measure of how often the term appears across the index 3. coord = number of terms in the query that were found in the document 4. lengthNorm = measure of the importance of a term according to the total number of terms in the field 5. queryNorm = normalization factor so that queries can be compared 6. boost (index) = boost of the field at index-time 7. boost (query) = boost of the field at query-time The implementation, implication and rationales of factors 1,2, 3 and 4 in DefaultSimilarity.java, which is what you get if you don't explicitly specify a similarity, are: note: the implication of these factors should be read as, \"Everything else being equal, … \" tf Implementation: sqrt(freq) Implication: the more frequent a term occurs in a document, the greater its score Rationale: documents which contains more of a term are generally more relevant idf Implementation: log(numDocs/(docFreq+1)) + 1 Implication: the greater the occurrence of a term in different documents, the lower its score Rationale: common terms are less important than uncommon ones coord Implementation: overlap / maxOverlap Implication: of the terms in the query, a document that contains more terms will have a higher score Rationale: self-explanatory lengthNorm Implementation: 1/sqrt(numTerms) Implication: a term matched in fields with less terms have a higher score Rationale: a term in a field with less terms is more important than one with more queryNorm is not related to the relevance of the document, but rather tries to make scores between different queries comparable. It is implemented as 1/sqrt(sumOfSquaredWeights) So, roughly speaking (quoting Mark Harwood from the mailing list), Documents containing all the search terms are good Matches on rare words are better than for common words Long documents are not as good as short ones Documents which mention the search terms many times are good The mathematical definition of the scoring can be found in org.apache.lucene.search.Class Similarity Customizing scoring Its easy to customize the scoring algorithm. Just subclass DefaultSimilarity and override the method you want to customize. For example, if you want to ignore how common a term appears across the index, Similarity sim = new DefaultSimilarity () { public float idf ( int i , int i1 ) { return 1 ; } } and if you think for the title field, more terms is better Similarity sim = new DefaultSimilarity () { public float lengthNorm ( String field , int numTerms ) { if ( field . equals ( \"title\" )) return ( float ) ( 0.1 * Math . log ( numTerms )); else return super . lengthNorm ( field , numTerms ); } }","title":"Lucene scoring examplained","url":"Lucene_scoring_examplained.html","tags":"Tutorial"},{"text":"Here is a quick example: $ grep -r \"text string to search\" directory-path To search for a string ‘logged in' in all text (*.log) files located in /etc/networks/ directory for example, use: $ grep \"logged in\" /etc/networks/*.log To search all subdirectories recursively, include -r option like so: $ grep -r \"logged in\" /etc/networks/*.log The grep command prints the matching lines for each match. Pass -H option to print the filename only: $ grep -H -r \"logged in\" /etc/networks/*.log To search for two or more words, use egrep: egrep -w -r 'logged in|logged out' /etc/networks/*.log To hide warning spam of permission for certain directories being denied, etc, send them to dev/null: $ grep -w -r 'logged in|logged out' /etc/networks/*.log 2 >/dev/null To make it case insensitive, use -i option: $ grep -i \"logged in\" /etc/networks/*.log","title":"How to find a file containing particular text in Linux","url":"How_to_find_a_file_containing_particular_text_in_Linux.html","tags":"How To article"}]}