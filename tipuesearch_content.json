{"pages":[{"tags":"pages","url":"pages/about.html","title":"About","text":"Why I started this blog My background My family My hobbies"},{"tags":"data prep","url":"featuretools_data_generation.html","title":"Data Creation for Automated Feature Engineering via Featuretools post","text":"How to generate sample data for Automated Feature Engineering via Featuretools post Introduction: Data Creation In this notebook we create an example dataset to be used for automated feature engineering. I have included this code in the repository for posterity and because at some point it may come in use for generating additional example datasets. In [1]: import pandas as pd import numpy as np from datetime import datetime import random rand_dates = [] for _ in range ( 1000 ): year = random . choice ( range ( 2000 , 2015 )) month = random . choice ( range ( 1 , 13 )) day = random . choice ( range ( 1 , 29 )) rdate = datetime ( year , month , day ) rand_dates . append ( rdate ) In [2]: clients = pd . DataFrame ( columns = [ 'client_id' , 'joined' , 'income' , 'credit_score' ]) for _ in range ( 25 ): clients = clients . append ( pd . DataFrame ({ 'client_id' : np . random . randint ( 25000 , 50000 , size = 1 )[ 0 ], 'joined' : random . choice ( rand_dates ), 'income' : np . random . randint ( 30500 , 240000 , size = 1 )[ 0 ], 'credit_score' : np . random . randint ( 500 , 850 , size = 1 )[ 0 ]}, index = [ 0 ]), ignore_index = True ) clients . head () Out[2]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } client_id joined income credit_score 0 46109 2002-04-16 172677 527 1 49545 2007-11-14 104564 770 2 41480 2013-03-11 122607 585 3 46180 2001-11-06 43851 562 4 25707 2006-10-06 211422 621 In [3]: loans = pd . DataFrame ( columns = [ 'client_id' , 'loan_type' , 'loan_amount' , 'repaid' , 'loan_id' , 'loan_start' , 'loan_end' , 'rate' ]) for client in clients [ 'client_id' ] . unique (): for _ in range ( 20 ): time_created = pd . datetime ( np . random . randint ( 2000 , 2015 , size = 1 )[ 0 ], np . random . randint ( 1 , 13 , size = 1 )[ 0 ], np . random . randint ( 1 , 30 , size = 1 )[ 0 ]) time_ended = time_created + pd . Timedelta ( days = np . random . randint ( 500 , 1000 , size = 1 )[ 0 ]) loans = loans . append ( pd . DataFrame ({ 'client_id' : client , 'loan_type' : random . choice ([ 'cash' , 'credit' , 'home' , 'other' ]), 'loan_amount' : np . random . randint ( 500 , 15000 , size = 1 )[ 0 ], 'repaid' : random . choice ([ 0 , 1 ]), 'loan_id' : np . random . randint ( 10000 , 12000 , size = 1 )[ 0 ], 'loan_start' : time_created , 'loan_end' : time_ended , 'rate' : round ( abs ( 4 * np . random . randn ( 1 )[ 0 ]), 2 )}, index = [ 0 ]), ignore_index = True ) In [4]: loans . head () Out[4]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } client_id loan_type loan_amount repaid loan_id loan_start loan_end rate 0 46109 home 13672 0 10243 2002-04-16 2003-12-20 2.15 1 46109 credit 9794 0 10984 2003-10-21 2005-07-17 1.25 2 46109 home 12734 1 10990 2006-02-01 2007-07-05 0.68 3 46109 cash 12518 1 10596 2010-12-08 2013-05-05 1.24 4 46109 credit 14049 1 11415 2010-07-07 2012-05-21 3.13 In [5]: payments = pd . DataFrame ( columns = [ 'loan_id' , 'payment_amount' , 'payment_date' , 'missed' ]) for _ , row in loans . iterrows (): time_created = row [ 'loan_start' ] payment_date = time_created + pd . Timedelta ( days = 30 ) loan_amount = row [ 'loan_amount' ] loan_id = row [ 'loan_id' ] payment_id = np . random . randint ( 10000 , 12000 , size = 1 )[ 0 ] for _ in range ( np . random . randint ( 5 , 10 , size = 1 )[ 0 ]): payment_id += 1 payment_date += pd . Timedelta ( days = np . random . randint ( 10 , 50 , size = 1 )[ 0 ]) payments = payments . append ( pd . DataFrame ({ 'loan_id' : loan_id , 'payment_amount' : np . random . randint ( int ( loan_amount / 10 ), int ( loan_amount / 5 ), size = 1 )[ 0 ], 'payment_date' : payment_date , 'missed' : random . choice ([ 0 , 1 ])}, index = [ 0 ]), ignore_index = True ) In [6]: payments . head () Out[6]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loan_id payment_amount payment_date missed 0 10243 2369 2002-05-31 1 1 10243 2439 2002-06-18 1 2 10243 2662 2002-06-29 0 3 10243 2268 2002-07-20 0 4 10243 2027 2002-07-31 1 In [7]: clients = clients . drop_duplicates ( subset = 'client_id' ) loans = loans . drop_duplicates ( subset = 'loan_id' ) clients . to_csv ( 'clients.csv' , index = False ) loans . to_csv ( 'loans.csv' , index = False ) payments . to_csv ( 'payments.csv' , index = False )"},{"tags":"data science","url":"featuretools.html","title":"Automated Feature Engineering via Featuretools","text":"Take aways Feature engineering, also known as feature creation, is the process of constructing new features from existing data to train a machine learning model. Feature engineering means building additional features out of existing data which is often spread across multiple related tables. Feature engineering requires extracting the relevant information from the data and getting it into a single table which can then be used to train a machine learning model. We can group the operations of feature creation into two categories: transformations and aggregations. A transformation acts on a single table by creating new features out of one or more of the existing columns. An aggregations are performed across tables, and use a one-to-many relationship to group observations and then calculate statistics. Aggregation operations are not difficult by themselves, but if we have hundreds of variables spread across dozens of tables, this process is not feasible to do by hand. Fortunately, featuretools is exactly the solution we are looking for. This open-source Python library will automatically create many features from a set of related tables. Introduction: Automated Feature Engineering In this notebook, we will look at an exciting development in data science: automated feature engineering. A machine learning model can only learn from the data we give it, and making sure that data is relevant to the task is one of the most crucial steps in the machine learning pipeline (this is made clear in the excellent paper \"A Few Useful Things to Know about Machine Learning\" ). However, manual feature engineering is a tedious task and is limited by both human imagination - there are only so many features we can think to create - and by time - creating new features is time-intensive. Ideally, there would be an objective method to create an array of diverse new candidate features that we can then use for a machine learning task. This process is meant to not replace the data scientist, but to make her job easier and allowing her to supplement domain knowledge with an automated workflow. In this notebook, we will walk through an implementation of using Featuretools , an open-source Python library for automatically creating features with relational data (where the data is in structured tables). Although there are now many efforts working to enable automated model selection and hyperparameter tuning, there has been a lack of automating work on the feature engineering aspect of the pipeline. This library seeks to close that gap and the general methodology has been proven effective in both machine learning competitions with the data science machine and business use cases . Dataset To show the basic idea of featuretools we will use an example dataset consisting of three tables: clients : information about clients at a credit union loans : previous loans taken out by the clients payments : payments made/missed on the previous loans The general problem of feature engineering is taking disparate data, often distributed across multiple tables, and combining it into a single table that can be used for training a machine learning model. Featuretools has the ability to do this for us, creating many new candidate features with minimal effort. These features are combined into a single table that can then be passed on to our model. First, let's load in the data and look at the problem we are working with. In [1]: # Run this if featuretools is not already installed # !pip install -U featuretools In [2]: # pandas and numpy for data manipulation import pandas as pd import numpy as np # featuretools for automated feature engineering import featuretools as ft # ignore warnings from pandas import warnings warnings . filterwarnings ( 'ignore' ) In [3]: # Read in the data clients = pd . read_csv ( 'data/clients.csv' , parse_dates = [ 'joined' ]) loans = pd . read_csv ( 'data/loans.csv' , parse_dates = [ 'loan_start' , 'loan_end' ]) payments = pd . read_csv ( 'data/payments.csv' , parse_dates = [ 'payment_date' ]) In [4]: clients . head () Out[4]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } client_id joined income credit_score 0 46109 2002-04-16 172677 527 1 49545 2007-11-14 104564 770 2 41480 2013-03-11 122607 585 3 46180 2001-11-06 43851 562 4 25707 2006-10-06 211422 621 In [5]: loans . sample ( 10 ) Out[5]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } client_id loan_type loan_amount repaid loan_id loan_start loan_end rate 254 29841 cash 4718 0 11175 2014-03-08 2015-09-13 2.74 196 26326 other 10902 1 11559 2014-09-06 2016-11-14 6.73 45 41480 cash 12581 1 11461 2009-10-29 2011-07-08 1.94 415 49624 cash 8621 0 10454 2014-07-26 2015-12-29 3.18 26 49545 other 4131 0 10939 2008-10-13 2010-02-26 4.07 417 49624 credit 11219 1 11132 2013-08-12 2016-03-15 3.77 137 32726 credit 7499 1 10285 2012-10-07 2015-03-12 2.72 200 26326 credit 5275 0 11072 2014-11-11 2016-07-17 1.45 180 48177 cash 6521 0 11235 2014-07-12 2016-02-22 6.17 8 46109 home 11062 1 11611 2012-09-12 2014-03-14 5.48 In [6]: payments . sample ( 10 ) Out[6]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loan_id payment_amount payment_date missed 965 11221 1511 2009-07-14 1 1285 11264 218 2009-11-06 1 1664 10769 210 2012-03-18 1 1074 10106 1003 2003-09-09 1 2521 10868 349 2013-10-04 1 2909 10537 1292 2005-11-23 1 44 11141 1220 2007-09-20 0 2445 10109 806 2014-07-23 0 1581 10624 1176 2001-03-20 1 1344 10262 770 2007-03-02 1 Manual Feature Engineering Examples Let's show a few examples of features we might make by hand. We will keep this relatively simple to avoid doing too much work! First we will focus on a single dataframe before combining them together. In the clients dataframe, we can take the month of the joined column and the natural log of the income column. Later, we see these are known in featuretools as transformation feature primitives because they act on column in a single table. In [7]: # Create a month column clients [ 'join_month' ] = clients [ 'joined' ] . dt . month # Create a log of income column clients [ 'log_income' ] = np . log ( clients [ 'income' ]) clients . head () Out[7]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } client_id joined income credit_score join_month log_income 0 46109 2002-04-16 172677 527 4 12.059178 1 49545 2007-11-14 104564 770 11 11.557555 2 41480 2013-03-11 122607 585 3 11.716739 3 46180 2001-11-06 43851 562 11 10.688553 4 25707 2006-10-06 211422 621 10 12.261611 To incorporate information about the other tables, we use the df.groupby method, followed by a suitable aggregation function, followed by df.merge . For example, let's calculate the average, minimum, and maximum amount of previous loans for each client. In the terms of featuretools, this would be considered an aggregation feature primitive because we using multiple tables in a one-to-many relationship to calculate aggregation figures (don't worry, this will be explained shortly!). In [8]: # Groupby client id and calculate mean, max, min previous loan size stats = loans . groupby ( 'client_id' )[ 'loan_amount' ] . agg ([ 'mean' , 'max' , 'min' ]) stats . columns = [ 'mean_loan_amount' , 'max_loan_amount' , 'min_loan_amount' ] stats . head () Out[8]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_loan_amount max_loan_amount min_loan_amount client_id 25707 7963.950000 13913 1212 26326 7270.062500 13464 1164 26695 7824.722222 14865 2389 26945 7125.933333 14593 653 29841 9813.000000 14837 2778 In [9]: # Merge with the clients dataframe clients . merge ( stats , left_on = 'client_id' , right_index = True , how = 'left' ) . head ( 10 ) Out[9]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } client_id joined income credit_score join_month log_income mean_loan_amount max_loan_amount min_loan_amount 0 46109 2002-04-16 172677 527 4 12.059178 8951.600000 14049 559 1 49545 2007-11-14 104564 770 11 11.557555 10289.300000 14971 3851 2 41480 2013-03-11 122607 585 3 11.716739 7894.850000 14399 811 3 46180 2001-11-06 43851 562 11 10.688553 7700.850000 14081 1607 4 25707 2006-10-06 211422 621 10 12.261611 7963.950000 13913 1212 5 39505 2011-10-14 153873 610 10 11.943883 7424.050000 14575 904 6 32726 2006-05-01 235705 730 5 12.370336 6633.263158 14802 851 7 35089 2010-03-01 131176 771 3 11.784295 6939.200000 13194 773 8 35214 2003-08-08 95849 696 8 11.470529 7173.555556 14767 667 9 48177 2008-06-09 190632 769 6 12.158100 7424.368421 14740 659 We could go further and include information about payments in the clients dataframe. To do so, we would have to group payments by the loan_id , merge it with the loans , group the resulting dataframe by the client_id , and then merge it into the clients dataframe. This would allow us to include information about previous payments for each client. Clearly, this process of manual feature engineering can grow quite tedious with many columns and multiple tables and I certainly don't want to have to do this process by hand! Luckily, featuretools can automatically perform this entire process and will create more features than we would have ever thought of. Although I love pandas , there is only so much manual data manipulation I'm willing to stand! Featuretools Now that we know what we are trying to avoid (tedious manual feature engineering), let's figure out how to automate this process. Featuretools operates on an idea known as Deep Feature Synthesis . You can read the original paper here , and although it's quite readable, it's not necessary to understand the details to do automated feature engineering. The concept of Deep Feature Synthesis is to use basic building blocks known as feature primitives (like the transformations and aggregations done above) that can be stacked on top of each other to form new features. The depth of a \"deep feature\" is equal to the number of stacked primitives. I threw out some terms there, but don't worry because we'll cover them as we go. Featuretools builds on simple ideas to create a powerful method, and we will build up our understanding in much the same way. The first part of Featuretools to understand is an entity . This is simply a table, or in pandas , a DataFrame . We corral multiple entities into a single object called an EntitySet . This is just a large data structure composed of many individual entities and the relationships between them. EntitySet Creating a new EntitySet is pretty simple: In [10]: es = ft . EntitySet ( id = 'clients' ) Entities An entity is simply a table, which is represented in Pandas as a dataframe . Each entity must have a uniquely identifying column, known as an index. For the clients dataframe, this is the client_id because each id only appears once in the clients data. In the loans dataframe, client_id is not an index because each id might appear more than once. The index for this dataframe is instead loan_id . When we create an entity in featuretools, we have to identify which column of the dataframe is the index. If the data does not have a unique index we can tell featuretools to make an index for the entity by passing in make_index = True and specifying a name for the index. If the data also has a uniquely identifying time index, we can pass that in as the time_index parameter. Featuretools will automatically infer the variable types (numeric, categorical, datetime) of the columns in our data, but we can also pass in specific datatypes to override this behavior. As an example, even though the repaid column in the loans dataframe is represented as an integer, we can tell featuretools that this is a categorical feature since it can only take on two discrete values. This is done using an integer with the variables as keys and the feature types as values. In the code below we create the three entities and add them to the EntitySet . The syntax is relatively straightforward with a few notes: for the payments dataframe we need to make an index, for the loans dataframe, we specify that repaid is a categorical variable, and for the payments dataframe, we specify that missed is a categorical feature. In [11]: # Create an entity from the client dataframe # This dataframe already has an index and a time index es = es . entity_from_dataframe ( entity_id = 'clients' , dataframe = clients , index = 'client_id' , time_index = 'joined' ) In [12]: # Create an entity from the loans dataframe # This dataframe already has an index and a time index es = es . entity_from_dataframe ( entity_id = 'loans' , dataframe = loans , variable_types = { 'repaid' : ft . variable_types . Categorical }, index = 'loan_id' , time_index = 'loan_start' ) In [13]: # Create an entity from the payments dataframe # This does not yet have a unique index es = es . entity_from_dataframe ( entity_id = 'payments' , dataframe = payments , variable_types = { 'missed' : ft . variable_types . Categorical }, make_index = True , index = 'payment_id' , time_index = 'payment_date' ) In [14]: es Out[14]: Entityset: clients Entities: clients [Rows: 25, Columns: 6] loans [Rows: 443, Columns: 8] payments [Rows: 3456, Columns: 5] Relationships: No relationships All three entities have been successfully added to the EntitySet . We can access any of the entities using Python dictionary syntax. In [15]: es [ 'loans' ] Out[15]: Entity: loans Variables: client_id (dtype: numeric) loan_type (dtype: categorical) loan_amount (dtype: numeric) loan_start (dtype: datetime_time_index) loan_end (dtype: datetime) rate (dtype: numeric) repaid (dtype: categorical) loan_id (dtype: index) Shape: (Rows: 443, Columns: 8) Featuretools correctly inferred each of the datatypes when we made this entity. We can also see that we overrode the type for the repaid feature, changing if from numeric to categorical. In [16]: es [ 'payments' ] Out[16]: Entity: payments Variables: loan_id (dtype: numeric) payment_amount (dtype: numeric) payment_date (dtype: datetime_time_index) missed (dtype: categorical) payment_id (dtype: index) Shape: (Rows: 3456, Columns: 5) Relationships After defining the entities (tables) in an EntitySet , we now need to tell featuretools how they are related with a relationship . The most intuitive way to think of relationships is with the parent to child analogy: a parent-to-child relationship is one-to-many because for each parent, there can be multiple children. The client dataframe is therefore the parent of the loans dataframe because while there is only one row for each client in the client dataframe, each client may have several previous loans covering multiple rows in the loans dataframe. Likewise, the loans dataframe is the parent of the payments dataframe because each loan will have multiple payments. These relationships are what allow us to group together datapoints using aggregation primitives and then create new features. As an example, we can group all of the previous loans associated with one client and find the average loan amount. We will discuss the features themselves more in a little bit, but for now let's define the relationships. To define relationships, we need to specify the parent variable and the child variable. This is the variable that links two entities together. In our example, the client and loans dataframes are linked together by the client_id column. Again, this is a parent to child relationship because for each client_id in the parent client dataframe, there may be multiple entries of the same client_id in the child loans dataframe. We codify relationships in the language of featuretools by specifying the parent variable and then the child variable. After creating a relationship, we add it to the EntitySet . In [17]: # Relationship between clients and previous loans r_client_previous = ft . Relationship ( es [ 'clients' ][ 'client_id' ], es [ 'loans' ][ 'client_id' ]) # Add the relationship to the entity set es = es . add_relationship ( r_client_previous ) The relationship has now been stored in the entity set. The second relationship is between the loans and payments . These two entities are related by the loan_id variable. In [18]: # Relationship between previous loans and previous payments r_payments = ft . Relationship ( es [ 'loans' ][ 'loan_id' ], es [ 'payments' ][ 'loan_id' ]) # Add the relationship to the entity set es = es . add_relationship ( r_payments ) es Out[18]: Entityset: clients Entities: clients [Rows: 25, Columns: 6] loans [Rows: 443, Columns: 8] payments [Rows: 3456, Columns: 5] Relationships: loans.client_id -> clients.client_id payments.loan_id -> loans.loan_id We now have our entities in an entityset along with the relationships between them. We can now start to making new features from all of the tables using stacks of feature primitives to form deep features. First, let's cover feature primitives. Feature Primitives A feature primitive a at a very high-level is an operation applied to data to create a feature. These represent very simple calculations that can be stacked on top of each other to create complex features. Feature primitives fall into two categories: Aggregation : function that groups together child datapoints for each parent and then calculates a statistic such as mean, min, max, or standard deviation. An example is calculating the maximum loan amount for each client. An aggregation works across multiple tables using relationships between tables. Transformation : an operation applied to one or more columns in a single table. An example would be extracting the day from dates, or finding the difference between two columns in one table. Let's take a look at feature primitives in featuretools. We can view the list of primitives: In [19]: primitives = ft . list_primitives () pd . options . display . max_colwidth = 100 primitives [ primitives [ 'type' ] == 'aggregation' ] . head ( 10 ) Out[19]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type description 0 mode aggregation Finds the most common element in a categorical feature. 1 any aggregation Test if any value is 'True'. 2 last aggregation Returns the last value. 3 all aggregation Test if all values are 'True'. 4 median aggregation Finds the median value of any feature with well-ordered values. 5 min aggregation Finds the minimum non-null value of a numeric feature. 6 avg_time_between aggregation Computes the average time between consecutive events. 7 max aggregation Finds the maximum non-null value of a numeric feature. 8 n_most_common aggregation Finds the N most common elements in a categorical feature. 9 percent_true aggregation Finds the percent of 'True' values in a boolean feature. In [20]: primitives [ primitives [ 'type' ] == 'transform' ] . head ( 10 ) Out[20]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type description 19 add transform Creates a transform feature that adds two features. 20 seconds transform Transform a Timedelta feature into the number of seconds. 21 time_since_previous transform Compute the time since the previous instance. 22 cum_min transform Calculates the min of previous values of an instance for each value in a time-dependent entity. 23 divide transform Creates a transform feature that divides two features. 24 hour transform Transform a Datetime feature into the hour. 25 and transform For two boolean values, determine if both values are 'True'. 26 or transform For two boolean values, determine if one value is 'True'. 27 years transform Transform a Timedelta feature into the number of years. 28 isin transform For each value of the base feature, checks whether it is in a provided list. If featuretools does not have enough primitives for us, we can also make our own. To get an idea of what a feature primitive actually does, let's try out a few on our data. Using primitives is surprisingly easy using the ft.dfs function (which stands for deep feature synthesis). In this function, we specify the entityset to use; the target_entity , which is the dataframe we want to make the features for (where the features end up); the agg_primitives which are the aggregation feature primitives; and the trans_primitives which are the transformation primitives to apply. In the following example, we are using the EntitySet we already created, the target entity is the clients dataframe because we want to make new features about each client, and then we specify a few aggregation and transformation primitives. In [21]: # Create new features using specified primitives features , feature_names = ft . dfs ( entityset = es , target_entity = 'clients' , agg_primitives = [ 'mean' , 'max' , 'percent_true' , 'last' ], trans_primitives = [ 'years' , 'month' , 'subtract' , 'divide' ]) In [22]: pd . DataFrame ( features [ 'MONTH(joined)' ] . head ()) Out[22]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MONTH(joined) client_id 25707 10 26326 5 26695 8 26945 11 29841 8 In [23]: pd . DataFrame ( features [ 'MEAN(payments.payment_amount)' ] . head ()) Out[23]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MEAN(payments.payment_amount) client_id 25707 1178.552795 26326 1166.736842 26695 1207.433824 26945 1109.473214 29841 1439.433333 In [24]: features . head () Out[24]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } income credit_score join_month log_income MEAN(loans.loan_amount) MEAN(loans.rate) MAX(loans.loan_amount) MAX(loans.rate) LAST(loans.loan_type) LAST(loans.loan_amount) ... log_income - income / MAX(payments.payment_amount) LAST(loans.loan_amount) / income log_income - credit_score / join_month join_month - log_income / MEAN(loans.loan_amount) join_month / income - join_month credit_score - log_income / LAST(loans.rate) join_month - log_income / MAX(loans.loan_amount) credit_score - income / join_month - log_income join_month - income / MEAN(payments.payment_amount) join_month - credit_score / log_income - credit_score client_id 25707 211422 621 10 12.261611 7963.950000 3.477000 13913 9.44 home 2203 ... -78.184075 0.010420 -60.873839 -0.000284 0.000047 82.261944 -0.000163 93208.319781 -179.382715 1.003715 26326 227920 633 5 12.336750 7270.062500 2.517500 13464 6.73 credit 5275 ... -85.744042 0.023144 -124.132650 -0.001009 0.000022 428.043621 -0.000545 30979.248435 -195.343964 1.011821 26695 174532 680 8 12.069863 7824.722222 2.466111 14865 6.51 other 13918 ... -59.522486 0.079745 -83.491267 -0.000520 0.000046 742.144596 -0.000274 42716.912967 -144.541255 1.006093 26945 214516 806 11 12.276140 7125.933333 2.855333 14593 5.65 cash 9249 ... -77.494120 0.043116 -72.156715 -0.000179 0.000051 277.525825 -0.000087 167466.003631 -193.339503 1.001608 29841 38354 523 8 10.554614 9813.000000 3.445000 14837 6.76 home 7223 ... -13.231003 0.188325 -64.055673 -0.000260 0.000209 100.676893 -0.000172 14808.890291 -26.639650 1.004985 5 rows × 797 columns Already we can see how useful featuretools is: it performed the same operations we did manually but also many more in addition. Examining the names of the features in the dataframe brings us to the final piece of the puzzle: deep features. Deep Feature Synthesis While feature primitives are useful by themselves, the main benefit of using featuretools arises when we stack primitives to get deep features. The depth of a feature is simply the number of primitives required to make a feature. So, a feature that relies on a single aggregation would be a deep feature with a depth of 1, a feature that stacks two primitives would have a depth of 2 and so on. The idea itself is lot simpler than the name \"deep feature synthesis\" implies. (I think the authors were trying to ride the way of deep neural network hype when they named the method!) To read more about deep feature synthesis, check out the documentation or the original paper by Max Kanter et al . Already in the dataframe we made by specifying the primitives manually we can see the idea of feature depth. For instance, the MEAN(loans.loan_amount) feature has a depth of 1 because it is made by applying a single aggregation primitive. This feature represents the average size of a client's previous loans. In [25]: # Show a feature with a depth of 1 pd . DataFrame ( features [ 'MEAN(loans.loan_amount)' ] . head ( 10 )) Out[25]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MEAN(loans.loan_amount) client_id 25707 7963.950000 26326 7270.062500 26695 7824.722222 26945 7125.933333 29841 9813.000000 32726 6633.263158 32885 9920.400000 32961 7882.235294 35089 6939.200000 35214 7173.555556 As well scroll through the features, we see a number of features with a depth of 2. For example, the LAST(loans.(MEAN(payments.payment_amount))) has depth = 2 because it is made by stacking two feature primitives, first an aggregation and then a transformation. This feature represents the average payment amount for the last (most recent) loan for each client. In [26]: # Show a feature with a depth of 2 pd . DataFrame ( features [ 'LAST(loans.MEAN(payments.payment_amount))' ] . head ( 10 )) Out[26]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LAST(loans.MEAN(payments.payment_amount)) client_id 25707 293.500000 26326 977.375000 26695 1769.166667 26945 1598.666667 29841 1125.500000 32726 799.500000 32885 1729.000000 32961 282.600000 35089 110.400000 35214 1410.250000 We can create features of arbitrary depth by stacking more primitives. However, when I have used featuretools I've never gone beyond a depth of 2! After this point, the features become very convoluted to understand. I'd encourage anyone interested to experiment with increasing the depth (maybe for a real problem) and see if there is value to \"going deeper\". Automated Deep Feature Synthesis In addition to manually specifying aggregation and transformation feature primitives, we can let featuretools automatically generate many new features. We do this by making the same ft.dfs function call, but without passing in any primitives. We just set the max_depth parameter and featuretools will automatically try many all combinations of feature primitives to the ordered depth. When running on large datasets, this process can take quite a while, but for our example data, it will be relatively quick. For this call, we only need to specify the entityset , the target_entity (which will again be clients ), and the max_depth . In [27]: # Perform deep feature synthesis without specifying primitives features , feature_names = ft . dfs ( entityset = es , target_entity = 'clients' , max_depth = 2 ) In [28]: features . iloc [:, 4 :] . head () Out[28]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SUM(loans.loan_amount) SUM(loans.rate) STD(loans.loan_amount) STD(loans.rate) MAX(loans.loan_amount) MAX(loans.rate) SKEW(loans.loan_amount) SKEW(loans.rate) MIN(loans.loan_amount) MIN(loans.rate) ... NUM_UNIQUE(loans.WEEKDAY(loan_end)) MODE(loans.MODE(payments.missed)) MODE(loans.DAY(loan_start)) MODE(loans.DAY(loan_end)) MODE(loans.YEAR(loan_start)) MODE(loans.YEAR(loan_end)) MODE(loans.MONTH(loan_start)) MODE(loans.MONTH(loan_end)) MODE(loans.WEEKDAY(loan_start)) MODE(loans.WEEKDAY(loan_end)) client_id 25707 159279 69.54 4044.418728 2.421285 13913 9.44 -0.172074 0.679118 1212 0.33 ... 6 0 27 1 2010 2007 1 8 3 0 26326 116321 40.28 4254.149422 1.991819 13464 6.73 0.135246 1.067853 1164 0.50 ... 5 0 6 6 2003 2005 4 7 5 2 26695 140845 44.39 4078.228493 1.517660 14865 6.51 0.154467 0.820060 2389 0.22 ... 6 0 3 14 2003 2005 9 4 1 1 26945 106889 42.83 4389.555657 1.564795 14593 5.65 0.156534 -0.001998 653 0.13 ... 6 0 16 1 2002 2004 12 5 0 1 29841 176634 62.01 4090.630609 2.063092 14837 6.76 -0.212397 0.050600 2778 0.26 ... 7 1 1 15 2005 2007 3 2 5 1 5 rows × 90 columns Deep feature synthesis has created 90 new features out of the existing data! While we could have created all of these manually, I am glad to not have to write all that code by hand. The primary benefit of featuretools is that it creates features without any subjective human biases. Even a human with considerable domain knowledge will be limited by their imagination when making new features (not to mention time). Automated feature engineering is not limited by these factors (instead it's limited by computation time) and provides a good starting point for feature creation. This process likely will not remove the human contribution to feature engineering completely because a human can still use domain knowledge and machine learning expertise to select the most important features or build new features from those suggested by automated deep feature synthesis. Next Steps While automatic feature engineering solves one problem, it provides us with another problem: too many features! Although it's difficult to say which features will be important to a given machine learning task ahead of time, it's likely that not all of the features made by featuretools add value. In fact, having too many features is a significant issue in machine learning because it makes training a model much harder. The irrelevant features can drown out the important features , leaving a model unable to learn how to map the features to the target. This problem is known as the \"curse of dimensionality\" and is addressed through the process of feature reduction and selection , which means removing low-value features from the data. Defining which features are useful is an important problem where a data scientist can still add considerable value to the feature engineering task. Feature reduction will have to be another topic for another day! Conclusions In this notebook, we saw how to apply automated feature engineering to an example dataset. This is a powerful method which allows us to overcome the human limits of time and imagination to create many new features from multiple tables of data. Featuretools is built on the idea of deep feature synthesis, which means stacking multiple simple feature primitives - aggregations and transformations - to create new features. Feature engineering allows us to combine information across many tables into a single dataframe that we can then use for machine learning model training. Finally, the next step after creating all of these features is figuring out which ones are important. Featuretools is currently the only Python option for this process, but with the recent emphasis on automating aspects of the machine learning pipeline, other competitiors will probably enter the sphere. While the exact tools will change, the idea of automatically creating new features out of existing data will grow in importance. Staying up-to-date on methods such as automated feature engineering is crucial in the rapidly changing field of data science. Now go out there and find a problem on which to apply featuretools! For more information, check out the documentation for featuretools . Also, read about how featuretools is used in the real world by Feature Labs , the company behind the open-source library."},{"tags":"How To article","url":"how_to_move_elasticsearch_data_directory.html","title":"How to move ElasticSearch data directory","text":"I was currently working on a project where I had to ingest massive amounts of data into ElasticSearch and I defaulted to /var/lib/elasticsearch to store all of my data. I admit it was a mistake as I soon was starring at a 403 error from my cluster. { \"type\": \"cluster_block_exception\", \"reason\": \"blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];\" } After quick google search and verifying that I was truly low on space by issusing df -h command, I had to move my data. Below are the steps that I took 1. Determine current location of data files, if in doubt (curl \"localhost:9200/_nodes/settings?pretty=true\") 2. Figure out where you have space and create a directory in it, let's say mega_elasticdata_dir 3. Stop ElastiSearch service: systemctl stop elasticsearch.service 4. Navigate to the current data directory determined in step 1 and move or copy files to new location (cp -RP * /mega_elasticdata_dir/) 5. Change ownership on new directory to elasticsearch (sudo chown -R elasticsearch:elasticsearch /mega_elasticdata_dir) 6. Edit data path inside of /etc/elasticsearch/elasticsearch.yml by changing parameter \"path.data:\" 7. Start ElasticSearch service again (systemctl start elasticsearch.service) 8. Check ElasticSearch log for any errors (tail -500f /var/log/elasticsearch/elasticsearch.log) At this point, you have moved the data, but if you were to try to index any data, you would still get the same error message even now that you have plenty of space. This is because ElasticSearch has switched to read-only mode when it could not index more documents (when the hard drive got full). This is done to ensure availability for read-only queries. ElasticSearch will not switch back automatically and you have to reset the flag manually by sending following command curl -XPUT -H \"Content-Type: application/json\" http://localhost:9200/_all/_settings -d '{\"index.blocks.read_only_allow_delete\": null}' References: ElasticSearch Settings Disk Allocator"},{"tags":"How To article","url":"move_mariadb_data.html","title":"How to move MariaDB data to another location","text":"I was recently working on an instance of MariaDB where I loaded almost 200GB of data. My /var folder filled up almost to 90% and I got all sorts of warning logs about size limitations, etc. Before it turned into bigger issue, I decided to take proactive approach and move my data to another location. Useful command to check the size $ sudo du -sch * Verify current data directory location $ mysql -u root -p Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 4 Server version: 10.1.33-MariaDB MariaDB Server Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]> select @@datadir; +--------------------------+ | @@datadir | +--------------------------+ | /var/lib/mysql/ | +--------------------------+ 1 row in set (0.00 sec) MariaDB [(none)]> exit Bye Stop running mariadb instance $ sudo systemctl stop mariadb $ sudo systemctl status mariadb Double check if instance is running (Force kill any rogue mysql processes) and check if port 3306 is still occupied $ ps -elf | grep mysql $ netstat -apn | grep 3306 Move data to new location and back up old instance $ sudo rsync -av /var/lib/mysql /mnt/big_data $ sudo mv /var/lib/mysql /var/lib/mysql.bak Edit configuration files to change the defaults. \"/etc/my.cnf\" [mysqld] datadir=/mnt/big_data/mysql/ socket=/mnt/big_data/mysql/mysql.sock \"/etc/my.cnf.d/client.cnf\" [client] port=3306 socket=/mnt/big_data/mysql/mysql.sock I also had to chmod new directory, otherwise mariadb was failing since it could not write to the new location. Start it back up and make sure that new directory is in use $ sudo systemctl start mariadb $ sudo systemctl status mariadb $ mysql -u root -p MariaDB [(none)]> select @@datadir; +--------------------------+ | @@datadir | +--------------------------+ | /mnt/big_data/mysql/ | +--------------------------+ 1 row in set (0.00 sec) Partial Reference"},{"tags":"How To article","url":"install_chrome_on_linux.html","title":"How to install Google Chrome on your Linux System","text":"Enable Google Yum repository Create a file called /etc/yum.repos.d/google-chrome.repo and add the following lines of code to it. [google-chrome] name = google-chrome baseurl = http://dl.google.com/linux/chrome/rpm/stable/$basearch enabled = 1 gpgcheck = 1 gpgkey = https://dl-ssl.google.com/linux/linux_signing_key.pub Install key -Refer to this guide for more details. $ wget -- no - check - certificate https : // dl . google . com / linux / linux_signing_key . pub $ sudo rpm -- import linux_signing_key.pub Install Google Chrome $ sudo yum install google-chrome-stable Start Google Chrome $ google-chrome &"},{"tags":"Reference","url":"layman_random_forrest.html","title":"Layman's Introduction to Random Forests","text":"Suppose you're very indecisive, so whenever you want to watch a movie, you ask your friend Willow if she thinks you'll like it. In order to answer, Willow first needs to figure out what movies you like, so you give her a bunch of movies and tell her whether you liked each one or not (i.e., you give her a labeled training set). Then, when you ask her if she thinks you'll like movie X or not, she plays a 20 questions-like game with IMDB, asking questions like \"Is X a romantic movie?\", \"Does Johnny Depp star in X?\", and so on. She asks more informative questions first (i.e., she maximizes the information gain of each question), and gives you a yes/no answer at the end. Thus, Willow is a decision tree for your movie preferences. But Willow is only human, so she doesn't always generalize your preferences very well (i.e., she overfits). In order to get more accurate recommendations, you'd like to ask a bunch of your friends, and watch movie X if most of them say they think you'll like it. That is, instead of asking only Willow, you want to ask Woody, Apple, and Cartman as well, and they vote on whether you'll like a movie (i.e., you build an ensemble classifier , aka a forest in this case). Now you don't want each of your friends to do the same thing and give you the same answer, so you first give each of them slightly different data. After all, you're not absolutely sure of your preferences yourself you told Willow you loved Titanic, but maybe you were just happy that day because it was your birthday, so maybe some of your friends shouldn't use the fact that you liked Titanic in making their recommendations. Or maybe you told her you loved Cinderella, but actually you really really loved it, so some of your friends should give Cinderella more weight. So instead of giving your friends the same data you gave Willow, you give them slightly perturbed versions. You don't change your love/hate decisions, you just say you love/hate some movies a little more or less (formally, you give each of your friends a bootstrapped version of your original training data). For example, whereas you told Willow that you liked Black Swan and Harry Potter and disliked Avatar, you tell Woody that you liked Black Swan so much you watched it twice, you disliked Avatar, and don't mention Harry Potter at all. By using this ensemble, you hope that while each of your friends gives somewhat idiosyncratic recommendations (Willow thinks you like vampire movies more than you do, Woody thinks you like Pixar movies, and Cartman thinks you just hate everything), the errors get canceled out in the majority. Thus, your friends now form a bagged (bootstrap aggregated) forest of your movie preferences. There's still one problem with your data, however. While you loved both Titanic and Inception, it wasn't because you like movies that star Leonardio DiCaprio. Maybe you liked both movies for other reasons. Thus, you don't want your friends to all base their recommendations on whether Leo is in a movie or not. So when each friend asks IMDB a question, only a random subset of the possible questions is allowed (i.e., when you're building a decision tree, at each node you use some randomness in selecting the attribute to split on , say by randomly selecting an attribute or by selecting an attribute from a random subset). This means your friends aren't allowed to ask whether Leonardo DiCaprio is in the movie whenever they want. So whereas previously you injected randomness at the data level, by perturbing your movie preferences slightly, now you're injecting randomness at the model level, by making your friends ask different questions at different times. And so your friends now form a random forest ."},{"tags":"How To article","url":"install_spark.html","title":"How to install Spark and use it from python via pyspark","text":"Visit Spark to download tgz version of spark with hadoop Unzip and move it to /opt directory $ tar -xzf spark-2.3.0-bin-hadoop2.7.tgz $ mv spark-2.3.0-bin-hadoop2.7 /opt/spark-2.3.0 Create sym link $ ln -s /opt/spark-2.3.0 /opt/spark Add it to bash export SPARK_HOME=/opt/spark export PATH=$SPARK_HOME/bin:$PATH At this point Spark is installed on your machine. Test it $ pyspark Welcome to ____ __ / __/__ ___ _____/ /__ _ \\ \\/ _ \\/ _ ` / __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.3.0 /_/ Using Python version 3.6.4 (default, Jan 16 2018 18:10:19) SparkSession available as ' spark ' . Connect it to your python scripts, by installing findspark to point python to the location of the Spark $ pip install findspark Install pyspark to be able to use Spark from python $ pip install pyspark Now you will be able to use Spark from your python scripts import findspark import pyspark from pyspark.sql import SQLContext , Row findspark . init ( spark_home = \"/opt/spark\" ) conf = pyspark . SparkConf () . setAppName ( 'tf_fraud' ) sc = pyspark . SparkContext ( conf = conf ) sqlctx = SQLContext ( sc ) connection_url = \"jdbc:oracle:thin:\" + username + \"/\" + password + \"@\" + ip + \":\" + str ( port ) + \"/\" + SID df_pyspark = sqlctx . read . format ( 'jdbc' ) . options ( url = connection_url , dbtable = \"employee\" , driver = \"oracle.jdbc.OracleDriver\" ) . load () print ( df_pyspark . printSchema ()) cad_ser_nums = df_pyspark . foreach ( print )"},{"tags":"How To article","url":"connect_to_oracle_via_python.html","title":"How to connect to remote Oracle database from your python script","text":"Not sure if this step is needed since Oracle Instant Client suppose to be back compatible, but I went ahead and installed client that matched Oracle instance that I was trying to connect to Determine Oracle version by running this command. Mine was 12.1.0.2.0 SELECT * FROM V$VERSION Download Oracle Instant Client . I have Linux 86-64 so I used this link to download RPM version. Install downloaded oracle instant client sudo yum install oracle-instantclient12.1-basic-12.1.0.2.0-1.x86_64.rpm Add oracle instant client to your PATH. Permanently since I don't have any other Oracle products that might have broken.: sudo sh -c \"echo /usr/lib/oracle/12.1/client64/lib > /etc/ld.so.conf.d/oracle-instantclient.conf\" sudo ldconfig At this point your environment is set. Now it is time to connect to it by installing cx_Oracle library in your python. python -m pip install cx_Oracle --upgrade That should do it. Now write a simple python script to get the version of the Oracle database that you are trying to connect to. import cx_Oracle ip = 'specify_ip_or_hostname' port = 1234 SID = 'specify_sid_or_schema' dsn_tns = cx_Oracle . makedsn ( ip , port , SID ) # You might get these via environment variable to make thing secure username = 'username' password = 'password' conn = cx_Oracle . connect ( username , password , dsn_tns ) print ( conn . version ) conn . close () >>> 12.1.0.2.0 References: Installing cx_Oracle on Linux Quick cx_Oracle Install"},{"tags":"Python","url":"mapping_sql_to_pandas.html","title":"Mapping SQL to Pandas","text":".table-borders td, .table-borders th { border: 1px solid black; padding: 10px; } span.bold-red { color: red; font-weight: bold; } SQL Pandas select * from table_name df select * from table_name limit 3 df.head(3) select col_name_1 from table_name where col_name_2 = 'value' df[df.col_name_2 == 'value'].col_name_1 select distinct col_name_1 from table_name df.col_name_1.unique() select * from table_name where col_name_1 = 'val_1' and col_name_2 = 'val_2' df[(df.col_name_1 == 'val_1') & (df.col_name_2 == 'val_2')] select col_name_1, col_name_3, col_name_3 from table_name where col_name_4 = 'val_1' and col_name_5 = 'val_2' df[(df.col_name_4 == 'val_1') & (df.col_name_5 == 'val_2')][['col_name_1', 'col_name_2', 'col_name_3']] select * from table_name where col_name_1 = 'value' order by col_name_2 df[df.col_name_1 == 'value'].sort_values('col_name_2') select * from table_name where col_name_1 = 'value' order by col_name_2 desc df[df.col_name_1 == 'value'].sort_values('col_name_2', ascending=False) select * from table_name where col_name in ('val_1', 'val_2') df[df.col_name.isin(['val_1', 'val_2'])] select * from table_name where col_name not in ('val_1', 'val_2') df[~df.col_name.isin(['val_1', 'val_2'])] select col_name_1, col_name_2, count(&ast;) from table_name group by col_name_1, col_name_2 order by col_name_1, col_name_2 df.groupby(['col_name_1', 'col_name_2']).size() select col_name_1, col_name_2, count(&ast;) from table_name group by col_name_1, col_name_2 order by col_name_1, count(&ast;) desc df.groupby(['col_name_1', 'col_name_2']).size().to_frame('size').reset_index().sort_values(['col_name_1', 'size'], ascending=[True, False]) select col_name_1, count(&ast;) from table_name where col_name_2 = 'val_1' group by col_name_1 having count(&ast;) > 1000 order by count(&ast;) desc df[df.col_name_2 == 'val_1'].groupby('col_name_1').filter(lambda g: len(g) > 1000).groupby('col_name_1').size().sort_values(ascending=False) select col_name from table_name order by size desc limit 10 df.nlargest(10, columns='col_name') select col_name from table_name order by size desc limit 10 offset 10 df.nlargest( 20 , columns='col_name').tail(10) select max (col_name), min (col_name), mean (col_name), median (col_name) from table_name df.agg({'col_name': ['min', 'max', 'mean', 'median']}) select col_name_1, col_name_2, col_name_3, col_name_4 from table_name_1 join table_name_2 on table_name_1.col_name_id_1 = table_name_2.col_name_id_2 where table_name_2.col_name = 'val' df1.merge(df2[df2.col_name == 'val'][['col_name_id_2']], left_on='col_name_id_1', right_on='col_name_id_2', how='inner')[['col_name_1', 'col_name_2', 'col_name_3', 'col_name_4']] create table table_name (col_name_1 integer, col_name_2 text); insert into table_name values (1, 'val_1'); insert into table_name values (2, 'val_2'); insert into table_name values (3, 'val_3'); df1 = pd.DataFrame({'id': [1, 2], 'name': ['val_1', 'val_2']}) df2 = pd.DataFrame({'id': [3], 'name': ['val_3']}) pd.concat([df1, df2]).reset_index(drop=True) update table_name set col_name_1 = 'val_1' where col_name_2 == 'val_2' df.loc[df['col_name_2'] == 'val_2', 'col_name_1'] = 'val_1' delete from table_name where col_name = 'val' df = df[df.col_name != 'val' df.drop(df[df.col_name == 'val'].index)"},{"tags":"How To article","url":"fix_spelling_errors.html","title":"How to remove typos from entity names via fuzzywuzzy module in python.","text":"While cleaning data in csv file, it is often common to see entity name such as city, person name, organization, etc being misspelled a little slightly and hence not producing same type of statistic as you might want. For example, let's say that you try to collect stats on number of times user accessed your page. In that user name can be entered manually or it provided by different systems, then it might be different slight. Here I mean that we need to ensure that case is the same not to throw our stats and any spaces are cleaned as well just as a prelim step. Also, the username might be slightly misspelled with an extra dash, space or single character. Here lays the danger though, there can we two usernames that vary just by a character. I remember the days of hotmail and that you would often get emails sent to blackwolf to your black.wolf account. Gmail saw this as a problem and now it ignores periods in the emails to avoid this issue. What I am trying to say is that you need to use typos with care. Now how you do it? First of all, clean the text from spaces and make it lowercased. import pandas as pd # read in our dataset df = pd . read_csv ( \"my_data.csv\" ) # convert to lower case df [ 'username' ] = df [ 'username' ] . str . lower () # remove trailing white spaces df [ 'username' ] = df [ 'username' ] . str . strip () # let's take a look at list of unique usernames values username = df [ 'username' ] . unique () # sort them alphabetically and then take a closer look username . sort () print ( username ) We should see some usernames that seem too close of a match... let's try to find all usernames that are more that 90% close to each other and replace them to the most common name import fuzzywuzzy # function to replace rows in the provided column of the provided dataframe # that match the provided string above the provided ratio with the provided string def replace_matches_in_column ( df , column , string_to_match , min_ratio = 90 ): # get a list of unique strings strings = df [ column ] . unique () # get the top 10 closest matches to our input string matches = fuzzywuzzy . process . extract ( string_to_match , strings , limit = 10 , scorer = fuzzywuzzy . fuzz . token_sort_ratio ) # only get matches with a ratio > 90 close_matches = [ matches [ 0 ] for matches in matches if matches [ 1 ] >= min_ratio ] # get the rows of all the close matches in our dataframe rows_with_matches = df [ column ] . isin ( close_matches ) # replace all rows with close matches with the input matches df . loc [ rows_with_matches , column ] = string_to_match # let us know the function's done print ( \"All done!\" ) Now you can call it as such # use the function we just wrote to replace close matches to \"d.i khan\" with \"d.i khan\" replace_matches_in_column ( df = df , column = 'username' , string_to_match = \"black.wolf\" ) All done!"},{"tags":"How To article","url":"encoding_csv_file_python.html","title":"How to detect encoding of CSV file in python","text":"In my line of work, I have to deal with a lot of spreadsheets coming my way with different type of data. I don't control these csv files, hence I never know how they are being generated. If I were to simply read the file, I would often get something like that. UnicodeDecodeError Traceback (most recent call last) pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens() pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_with_dtype() pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._string_convert() pandas/_libs/parsers.pyx in pandas._libs.parsers._string_box_utf8() UnicodeDecodeError: 'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte Basically, when you specify the following, you assume that the information was encoded in UTF-8 ()default) format data = pd . read_csv ( \"my_data.csv\" ) However, if that's not the case and format is not UTF-8 then you get a nasty error shown previously. What to do? Try manually some common encoders, or look at the file and try to figure it out? A much better way is to use chardet module to do it for you. Here we going to read first ten thousand bytes to figure out the encoding type. Note that chardet is not 100% accurate and you would actually see the level of confidence of encoder detection as part of chardet output. But it is still better than guessing manually. # look at the first ten thousand bytes to guess the character encoding with open ( \"my_data.csv\" , 'rb' ) as rawdata : result = chardet . detect ( rawdata . read ( 10000 )) # check what the character encoding might be print ( result ) The result is {'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''} So chardet is 73% confidence that the right encoding is \"Windows-1252\". Now we can use this data to specify encoding type as we trying to read the file data = pd . read_csv ( \"my_data.csv\" , encoding = 'Windows-1252' )) No errors!"},{"tags":"How To article","url":"Commit_your_changes_or_stash_them_before_you_can_merge.html","title":"Commit your changes or stash them before you can merge??!?!","text":"When trying to update your local copy from remote master copy, you will see following error $ git pull origin master error: Your local changes to the following files would be overwritten by merge: <list of files> Please commit your changes or stash them before you merge. Aborting You have several options here Commit the change $ git add . $ git commit -m \"committing before the update\" Stash them $ git stash $ git stash pop Overwrite local changes git reset --hard"},{"tags":"How To article","url":"check_in_folder_ignore_contents.html","title":"How to check in a folder, but ignore its contents","text":"Git does not let you to check in an empty folder, even if you are using it as a temp output location. How to work around it? In the folder that you are trying to commit, create .gitignore file and add following content &ast; &ast;/ !.gitignore then add it to the git $ git add .gitignore The &ast; line tells git to ignore all files in the folder, but !.gitignore tells git to still include the .gitignore file. This way, your local repository and any other clones of the repository all get both the empty folder and the .gitignore it needs. May be obvious but also add &ast;/ to the git ignore to also ignore sub folders."},{"tags":"How To article","url":"back_it_off.html","title":"How to back off back to the earlier version of the code","text":"Say you are developing a new feature and you realize after few commits that you went off to a way different route that you suppose to and you need to back it up few commits and start over... this definitely would be a cleaner way vs trying to remove what was done manually. How to do it though? Check out what you want and get rid of all that code... $ git reset --hard 0d3b7ac32 Then you would push it up $ git push origin +master Pretty simple once you know it."},{"tags":"How To article","url":"How_to_remove_files_added_to_git.html","title":"How to remove files added to git","text":"Imagine the situation where you wrote your code and then decided to add it to your git repo. Pretty easy right? $ git init $ git add . Before you commit, you want to see what's going to be committed. So you do $ git status Now you see whole bunch of config and target files that have no business being in the repo. Not a problem, you can use .gitignore right? First remove what you added, create .gitignore file and you can re add again only source files. $ git rm -r . create .gitignore with /target/&ast;&ast; .settings/&ast;&ast; .classpath .project and re-add $ git add . Check what's about to be committed... and what?!?!? old files? How can this be? Did I messed up my regex? spelled gitignore wrong or forgot the leading period? Nope, everything seems correct... After reading gitignore help guide... you need to clear your cache!!! Here is what you do $ git rm -r --cached . cached flag is the key difference. After this command, re-add, verify and finally commit: $ git add . $ git commit -m \"source files only!!!\""},{"tags":"Hardware hacks","url":"Accessing_wired_Windows_printer_from_your_Mac.html","title":"Accessing wired Windows printer from your Mac","text":"Before my PC seized to be, I used to have old wired printer that did the job... the only problem was that I already have few Apple laptops and I wanted to be able to print from them (The problem is easily solved by buying wireless printer that supports AirPrint, i.e. even print from your iPhone!). Anyway, if you have wired Windows printer and don't want to upgrade just yet here is what you need to do: On Windows PC 1. Establish user account on your PC. This was one thing that I had to do to make everything that should work to actually work. This is as easy as opening your control panel and clicking on Add user in your Users menu. For more tricks see this: http://www.howtogeek.com/howto/10325/manage-user-accounts-in-windows-home-server/ 2. Now onto actual set up... Select Start->Devices and Printers. Right click on the printer that you want to share, and either pick share or properties and then pick sharing tab. Make sure that share check box is selected and make sure that you note down the name of the printer. 3. Open command prompt. Use ipconfig command to find your PC's IP. To summarize this part. You have IP address and name of the printer to connect to and you have the credentials that you created in step 1. On MAC 1. Open System Preferences and locate Printers and Scanners icon. Click! 2. Select + under printers to add new printer, i.e. wired Windows printer. 3. Right click on the menu and select Customize Toolbar and add Advanced 4. Click on Advanced. For type select Windows printer via spoolss 5. For URL provide IP and printer name that you have... so the link looks like smb://192.138.1.13/printer_name (Replace any spaces with %20 in your 6. Under Choose a driver or Printer Model pick your printer type (I did not see my exact model so I picked closes HP model instead. Worked) 7. At this point your PC printer will be connected to your Mac. Testing Select something to print on your Mac... in my case, the first time it tried to print it asked for my PC username/password which we created previously, after that it was stored in my keychain and was never an issue again. That's it! Hope it helps and once again, let me know if you have further questions, etc."},{"tags":"Tutorial","url":"Lucene_scoring_examplained.html","title":"Lucene scoring examplained","text":"Several good books already explain what Lucene scoring really means and how it is calculated in great detail with lots of basic concepts explain. In this, post I am going to try to keep it high level for people already familiar with the basics and go straight for the scoring overview. The factors involved in Lucene's scoring algorithm are as follows: 1. tf = term frequency in document = measure of how often a term appears in the document 2. idf = inverse document frequency = measure of how often the term appears across the index 3. coord = number of terms in the query that were found in the document 4. lengthNorm = measure of the importance of a term according to the total number of terms in the field 5. queryNorm = normalization factor so that queries can be compared 6. boost (index) = boost of the field at index-time 7. boost (query) = boost of the field at query-time The implementation, implication and rationales of factors 1,2, 3 and 4 in DefaultSimilarity.java, which is what you get if you don't explicitly specify a similarity, are: note: the implication of these factors should be read as, \"Everything else being equal, … \" tf Implementation: sqrt(freq) Implication: the more frequent a term occurs in a document, the greater its score Rationale: documents which contains more of a term are generally more relevant idf Implementation: log(numDocs/(docFreq+1)) + 1 Implication: the greater the occurrence of a term in different documents, the lower its score Rationale: common terms are less important than uncommon ones coord Implementation: overlap / maxOverlap Implication: of the terms in the query, a document that contains more terms will have a higher score Rationale: self-explanatory lengthNorm Implementation: 1/sqrt(numTerms) Implication: a term matched in fields with less terms have a higher score Rationale: a term in a field with less terms is more important than one with more queryNorm is not related to the relevance of the document, but rather tries to make scores between different queries comparable. It is implemented as 1/sqrt(sumOfSquaredWeights) So, roughly speaking (quoting Mark Harwood from the mailing list), Documents containing all the search terms are good Matches on rare words are better than for common words Long documents are not as good as short ones Documents which mention the search terms many times are good The mathematical definition of the scoring can be found in org.apache.lucene.search.Class Similarity Customizing scoring Its easy to customize the scoring algorithm. Just subclass DefaultSimilarity and override the method you want to customize. For example, if you want to ignore how common a term appears across the index, Similarity sim = new DefaultSimilarity () { public float idf ( int i , int i1 ) { return 1 ; } } and if you think for the title field, more terms is better Similarity sim = new DefaultSimilarity () { public float lengthNorm ( String field , int numTerms ) { if ( field . equals ( \"title\" )) return ( float ) ( 0.1 * Math . log ( numTerms )); else return super . lengthNorm ( field , numTerms ); } }"},{"tags":"How To article","url":"How_to_find_a_file_containing_particular_text_in_Linux.html","title":"How to find a file containing particular text in Linux","text":"Here is a quick example: $ grep -r \"text string to search\" directory-path To search for a string ‘logged in' in all text (*.log) files located in /etc/networks/ directory for example, use: $ grep \"logged in\" /etc/networks/*.log To search all subdirectories recursively, include -r option like so: $ grep -r \"logged in\" /etc/networks/*.log The grep command prints the matching lines for each match. Pass -H option to print the filename only: $ grep -H -r \"logged in\" /etc/networks/*.log To search for two or more words, use egrep: egrep -w -r 'logged in|logged out' /etc/networks/*.log To hide warning spam of permission for certain directories being denied, etc, send them to dev/null: $ grep -w -r 'logged in|logged out' /etc/networks/*.log 2 >/dev/null To make it case insensitive, use -i option: $ grep -i \"logged in\" /etc/networks/*.log"}]}